{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a virtual environment**\n",
    "\n",
    "If you are working on a lab computer, check whether there already is a virtual environment in `/var/vsv306`. In a terminal do:\n",
    "```\n",
    "ls /var/csc306\n",
    "```\n",
    "If you see a folder named `csc306.venv`, someone has already created a virtual environment on this computer.\n",
    "\n",
    "If not, run:\n",
    "```\n",
    "python3 -m venv /var/csc306/csc306.venv\n",
    "```\n",
    "\n",
    "(If you are working on your own computer, you can put the virtual environment wherever you want. Replace `/var/csc306` accordingly.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activate virtual environment**\n",
    "\n",
    "```\n",
    "source /var/csc306/csc306.venv/bin/activate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install libraries**\n",
    "```\n",
    "pip install ipykernel openai python-dotenv rich rank_bm25\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select a kernel**\n",
    "\n",
    "To execute cells in this notebook, you need to connect to a Python kernel. You want to use the virtual environment you created. In VS Code type `Ctrl-shift-p` and then `Python: select interpreter`. That should bring up a drop down menu. Choose \"Enter interpreter path\" and then \"Browse your file system to find a Python interpreter\". Navigate to `/var/csc306/csc306.venv/bin/python`. \n",
    "\n",
    "Now click \"Select Kernel\" in the top right of your VS Code editor panel. Then choose \"Python environments\". Your virtual environment should be one of the options.\n",
    "\n",
    "Now, you should be able to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello Python!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoreload imported Python files when they change**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executing this cell will ensure that imported modules (.py files) will automatically\n",
    "# be reloaded when they change. (However, objects that were defined with the old\n",
    "# version of the class won't change.)\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an OpenAI client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An OpenAI API key will be sent to you.\n",
    "\n",
    "Make an `.env` file in the same directory as this notebook, containing the following:\n",
    "```\n",
    "export OPENAI_API_KEY=[your API key]    # do not include the brackets here\n",
    "```\n",
    "Make sure others can't read this file:\n",
    "```\n",
    "chmod 600 .env\n",
    "```\n",
    "\n",
    "**Be sure to keep the key secret.  It gives access to a billable account.** If OpenAI finds it on the public web, they will invalidate it, and then no one (including you) can use this key to make requests anymore.\n",
    "\n",
    "Now you can execute the following to get an OpenAI client object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tracking import new_default_client, read_usage\n",
    "client = new_default_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That fetches your API key and calls `openai.OpenAI()` to make a new **client** object, whose job is to talk to the OpenAI **server** over HTTP.  (The `OpenAI` constructor has some optional arguments that configure these HTTP messages. However, the defaults should work fine for you.)\n",
    "\n",
    "That command also saved the new client in `tracking.default_client`, which is the client that the starter code will use by default whenever it needs to talk to the OpenAI server.  Thus, you should **rerun the above cell** to get a new client if you change the `default_model` in `tracking.py`, or if your API key in  `.env` ever changes, or its associated organization ever changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the model!\n",
    "\n",
    "You can now get answers from OpenAI models by calling methods of the `client` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function from class. Try it to make sure you can access the OpenAI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete(client, s: str, model=\"gpt-3.5-turbo-0125\", *args, **kwargs):\n",
    "    response = client.chat.completions.create(messages=[{\"role\": \"user\", \"content\": s}],\n",
    "                                              model=model,\n",
    "                                              *args, **kwargs)\n",
    "    return [choice.message.content for choice in response.choices]\n",
    "\n",
    "complete(client, \"I went to the store and I bought apples, bananas, cherries, donuts, eggs\",\n",
    "         n=10, temperature=0.6, max_tokens=96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute a function using instructions and few-shot prompting\n",
    "\n",
    "Let's try prompting the model with a sequence of multiple messages. In this case, we provide some instructions as well as few-shot prompting (actually just one-shot in this case).\n",
    "\n",
    "Instructions are in the `system` message. The few-shot prompting consists of example inputs (`user` messages) followed by their example outputs (`assistant` messages). Then we give our real input (the final `user` message), and hope that the LLM will continue the pattern by generating an analogous output (a new `assistant` message)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rich\n",
    "response = client.chat.completions.create(messages=[{ \"role\": \"system\",      # instructions\n",
    "                                                      \"content\": \"Reverse the order of the words.\" },\n",
    "                                                    { \"role\": \"user\",        # input\n",
    "                                                      \"content\": \"Good things come to those who wait.\" },\n",
    "                                                    { \"role\": \"assistant\",   # output\n",
    "                                                      \"content\": \"Wait who those to come things good.\" },\n",
    "                                                    { \"role\": \"user\",        # input\n",
    "                                                      \"content\": \"Colorless green ideas sleep furiously.\" }],\n",
    "                                          model=\"gpt-4o-mini\", temperature=0)\n",
    "#rich.print(response)\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modifying this call, can you get it to produce different versions of the output? Some possible behaviors you could try to arrange:\n",
    "\n",
    "* specific other way of formatting the output, e.g., wait, who, those, to, come, things, good\n",
    "* match the input's way of formatting the output (same use of capitalization, puncutation, commas)\n",
    "* reverse the phrases rather than reversing the words, e.g., To those who wait come good things.\n",
    "\n",
    "You can try playing with the number, the content, and the order of few-shot examples, and changing or removing the instructions.\n",
    "\n",
    "What happens if the examples conflict with the instructions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check your usage so far\n",
    "\n",
    "Please be careful not to write loops that use lots and lots of tokens. That will cost us money, and could hit the per-day usage limit that is shared by the whole class.\n",
    "\n",
    "Execute the cell below whenever you want to see your cost so far. Or, just open `usage_openai.json` as a tab in your IDE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dialogues and dialogue agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this assignment is to create a good \"argubot\" that will talk to people about controversial topics and broaden their minds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first argubot (Airhead)\n",
    "\n",
    "You can have a conversation right now with a really bad argubot named Airhead. Try asking it about climate change! When you're done, reply with an empty string.\n",
    "\n",
    "(The `converse()` method calls Python's `input()` function, which will prompt you for input at the command-line or by popping up a box in your IDE. In VS Code, the input box appears at the top edge of the window.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argubots\n",
    "d = argubots.airhead.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A bot (short for \"robot\") is a system that acts autonomously. That corresponds to the AI notion of an agent — a system that uses some policy to choose actions to take.\n",
    "\n",
    "The airhead agent above (defined in `argubots.py`) uses a particularly simple policy.\n",
    "It is an instance of a simple Agent subclass called `ConstantAgent` (defined in `agents.py`).\n",
    "\n",
    "The result of talking to airhead is a Dialogue object (defined in dialogue.py). Let's look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each turn of this dialogue is just a tiny dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An LLM argubot (Alice)\n",
    "\n",
    "In other CS courses, you may have encountered \"conversations\" between characters named Alice and Bob.\n",
    "\n",
    "Let's try talking to the Alice of this homework, who is a much stronger baseline than Airhead. Your job in this assignment is to improve upon Alice. We'll meet Bob later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call with argument d if you want to append to the previous conversation\n",
    "alicechat = argubots.alice.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may have guessed, `alice` is powered by a prompted LLM. You can find the specific prompt in `argubots.py`.\n",
    "\n",
    "So, while `agents.py` provides the core functionality for `Agent` objects, the argubot agents like `alice` -- and the ones that you will write! -- go into `argubots.py` instead. This is just to keep the files small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating human characters (Bob & friends)\n",
    "\n",
    "You'll talk to your own argubots to get a qualitative feeling for their strengths and weaknesses.\n",
    "But can you really be sure you're making progress? For that, a quantitative measure can be helpful.\n",
    "\n",
    "Ultimately, you should test an argubot like Alice by having it argue with many real humans — not just you — and using some rubric to score the resulting dialogues. But that would be slow and complicated to arrange.\n",
    "\n",
    "So, meet Bob! He's just a simulated human. You won't edit him: he is part of the development set. Here is some information about him (from `characters.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import characters\n",
    "rich.print(characters.bob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can't talk directly to `characters.bob` because that's just a data object. However, you can construct a simple agent that uses that data (plus a few more instructions) to prompt an LLM.\n",
    "\n",
    "(Which LLM does it prompt? The `CharacterAgent` constructor (defined in `agents.py`) defaults to gpt-4o-mini as specified in tracking.py. But you can override that using keyword arguments.)\n",
    "\n",
    "Try talking to Bob about climate change, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents import CharacterAgent\n",
    "# actually, agents.bob is already defined this way\n",
    "bob = CharacterAgent(characters.bob)\n",
    "# returns a dialogue, but we've already seen it so we don't want to print it again\n",
    "bob.converse()\n",
    "# don't print anything for this notebook cell\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, a proper user study can't just be conducted with one human user.\n",
    "\n",
    "So, meet our bevy of beautiful Bobs! (They're not actually all named Bob — we continued on in the alphabet.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import agents\n",
    "agents.devset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agents.cara.converse()\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the underlying character data here in the notebook. Your argubot will have to deal with all of these topics and styles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich.print(characters.devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating conversation\n",
    "\n",
    "We can make Alice and Bob chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dialogue import Dialogue\n",
    "d = Dialogue()                                              # empty dialogue\n",
    "d = d.add('Alice', \"Do you think it's okay to eat meat?\")   # add first turn\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = agents.bob.respond(d)\n",
    "d = argubots.alice.respond(d)\n",
    "rich.print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = agents.bob.respond(d)\n",
    "d = argubots.alice.respond(d)\n",
    "rich.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyway, let's see what happens when Alice and Bob talk for a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulate import simulated_dialogue\n",
    "d = simulated_dialogue(argubots.alice, agents.bob, 8)\n",
    "rich.print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes this kind of conversation seems to stall out, with Bob in particular repeating himself a lot. Alice doesn't seem to have a good strategy for getting him to open up. Maybe you can do a better job talking to Bob, and that will give you some ideas about how to improve Alice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your name, pulled from an earlier dialogue\n",
    "myname = alicechat[0]['speaker']\n",
    "# reuse the same first two turns, then type your own lines!\n",
    "agents.bob.converse(d[0:2].rename('Alice', myname))\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try talking to the other characters and having Alice (or Airhead) talk to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "❓❓❓ <b>Task 1: Define a simulated human character<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define an additional character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from characters import Character\n",
    "\n",
    "# See characters.py for how to use the Character class.\n",
    "# Add the definition of your character here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Please don't change the dev set — the characters we just loaded must stay the same. Your job in this homework is to improve the argubot (or at least try). And that means improving it according to a fixed and stable evaluation measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is our goal for the argubot? We'd like it to broaden the thinking of the (simulated) human that it is talking to. Indeed, that's what Alice's prompt tells Alice to do.\n",
    "\n",
    "This goal is inspired by the recent paper [Opening up Minds with Argumentative Dialogues](https://aclanthology.org/2022.findings-emnlp.335/), which collected human-human dialogues:\n",
    "\n",
    "> In this work, we focus on argumentative dialogues that aim to open up (rather than change) people’s minds to help them become more understanding to views that are unfamiliar or in opposition to their own convictions. ... Success of the dialogue is measured as the change in the participant’s stance towards those who hold opinions different to theirs.\n",
    "\n",
    "Arguments of this sort are not like chess or tennis games, with an actual winner. The argubot will almost never hear a human say \"You have convinced me that I was wrong.\" But the argubot did a good job if the human developed increased understanding and respect for an opposing point of view.\n",
    "\n",
    "To find out whether this happened, we can use a questionnaire to ask the human what they thought after the dialogue. For example, after Alice talks to Bob, we'll ask Bob to evaluate what he thinks of Alice's views. Of course, that depends on his personality -- Alice needs to talk to him in a way that reaches him (as much as possible). We'll also ask an outside observer to evaluate whether Alice handled the conversation with Bob well.\n",
    "\n",
    "Of course, we're still not going to use real humans. Bob is a fake person, and so is the outside observer (whose name is Judge Wise). Using an LLM as an eval metric is known as model-based evaluation. It has pros and cons:\n",
    "\n",
    "* It is cheaper, faster, and more replicable than hiring actual humans to do the evaluation.\n",
    "* It might give different answers than what humans would give.\n",
    "\n",
    "Social scientists usually refer to a metric's reliability (low variance) and validity (low bias). So the points above say that model-based evaluation is reliable but not necessarily valid. In general, an LLM-based metric (like any metric) needs to be validated to confirm that it really does measure what it claims to measure. (For example, that it correlates strongly with some other measure that we already trust.) In this homework, we'll skip this step and just pray that the metric is reasonable.\n",
    "\n",
    "To see how this works out in practice, **open up the `demo` notebook**, which walks you through the evaluation protocol. You'll see how to call the starter code, how it talks to the LLM behind the scenes, and what it is able to accomplish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "❓❓❓ <b>Task 2: Evaluate Alice and Airhead<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To  establish baselines for the character you are going to develop later on, evaluate the argubots Airhead and Alice using the model based evaluation strategy shown in `demo.py`. That is, use `evaluate.eval_on_characters`.\n",
    "\n",
    "This also helps to validate the metric. Airhead should get a low score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the code for your evaluation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the starter code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `demo` notebook gave you a good high-level picture of what the starter code is doing. So now you're probably curious about the details. Now that you've had the view from the top, here's a good bottom-up order in which to study the code. You don't need to understand every detail, but you will need to understand enough to call it and extend it.\n",
    "\n",
    "* `character.py`. The `Character` class is short and easy.\n",
    "\n",
    "* `dialogue.py`. The `Dialogue` class is meant to serve as a record of a natural-language conversation among any number of humans and/or agents. On each turn of the dialogue, one of the speakers says something.\n",
    "\n",
    "The dialogue's sequence of turns may remind you of the sequence of messages that is sent to OpenAI's chat completions API. But the OpenAI messages are only labeled with the 4 special roles `user`, `assistant`, `tool`, and `system`. Those are not quite the same thing as human speakers. And the OpenAI messages do not necessarily form a natural-language dialogue: some of the messages are dealing with instructions, few-shot prompting, tool use, and so on. The `agents.dialogue_to_openai` function in the next module will map a `Dialogue` to a (hopefully appropriate) sequence of messages for asking the LLM to extend that dialogue.\n",
    "\n",
    "* `agents.py`. This module sets up the problem of automatically predicting the next turn in a dialogue, by implementing an `Agent`'s `response()` method. The `Agent` base class also has some simple convenience methods that you should look at.\n",
    "\n",
    "Some important subclasses of `Agent` are defined here as well. However, you may want to skip over `EvaluationAgent` and come back to it only when you read `evaluate.py`.\n",
    "\n",
    "* `simulate.py` makes agents talk to one another, which we'll do during evaluation.\n",
    "\n",
    "* `argubots.py` starts to describe some useful agents. One of them makes use of the `kialo.py` module, which gives access to a database of arguments.\n",
    "\n",
    "* `evaluate.py` makes use of `simulate.simulated_dialogue` to `agents.EvaluationAgent` to evaluate an argubot.\n",
    "\n",
    "* We also have a couple of utility modules. These aren't about NLP; look inside if needed. `logging_cm.py` is what enabled the context manager with `LoggingContext(...)`: in the `demo` notebook. `tracking.py` sets some global defaults about how to use the OpenAI API, and arranges to track how many tokens we're paying for when you call it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity-based retrieval: Looking up relevant responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is fine to prompt an LLM to generate text, but there are other methods! There is a long history of machine learning methods that \"memorize\" the training data. To make a prediction or decision at test time, they consult the stored training examples that are most similar to the training situation.\n",
    "\n",
    "*Similarity-based retrieval* means that given a document x, you find the \"most similar\" documents y∈Y, where Y is a given collection of documents. The most common way to do this is to maximize the cosine similarity between the embeddings of x and y.\n",
    "\n",
    "A simple and fast approach is to use a bag of tokens embedding function: Define the embedding e(y) to be the vector that records the count of each type of token in a tokenized version of y, where V is the token vocabulary. BM25 is a refined variant of that idea, where the counts are adjusted in 3 ways:\n",
    "\n",
    "* smooth the counts\n",
    "* normalize for the document length |y| so that longer documents y are not more likely to be retrieved downweight tokens that are more common in the corpus (such as \"the\" or \"ing\") since they provide less information about the content of the document\n",
    "\n",
    "You might like to play with the rank_bm25 package (documentation). It is widely used and very easy to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi  # the standard BM25 method\n",
    "\n",
    "# experiment here!  You could try the examples in the rank_bm25 documentation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Kialo corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use similarity-based retrieval to help build an argubot? It's largely about having the right data!\n",
    "\n",
    "[Kialo](https://www.kialo.com/) is a collaboratively edited website (like Wikipedia) for discussing political and philosophical topics. For each topic, the contributors construct a tree of claims. Each claim is a natural-language sentence (usually), and each of its children is another claim that supports it (\"pro\") or opposes it (\"con\"). For example, check out the tree rooted at the claim []\"All humans should be vegan.\"](https://www.kialo.com/all-humans-should-be-vegan-2762).\n",
    "\n",
    "We provide a class `Kialo` for browsing a collection of such trees. Please read the source code in `kialo.py`. The class constructor reads in text files that are [exported Kialo discussions](https://support.kialo.com/en/hc/exporting-a-discussion/); we have provided some in the `data` directory. The class includes a BM25 index, to be able to find claims that are relevant to a given string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kialo import Kialo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's pull the retrieved discussions (the `.txt` files) into our data structure.\n",
    "\n",
    "For BM25 purposes, we have to be able to turn each document (that is, each Kialo claim) as a list of string or integer tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from typing import List\n",
    "import glob\n",
    "\n",
    "# use simple default tokenizer\n",
    "kialo = Kialo(glob.glob(\"data/*\"))\n",
    "f\"This Kialo subset contains {len(kialo)} claims\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.random_chain()   # just a single random claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.random_chain(n=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity-based retrieval from the Kialo corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it, using BM25!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.closest_claims(\"animal populations\", n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can restrict to claims for which the Kialo data structure has at least one counterargument (\"con\" child)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.closest_claims(\"animal populations\", n=10, kind='has_cons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = _[0]    # first claim above\n",
    "print(\"Parent claim:\\n\\t\" + str(kialo.parents[c]))\n",
    "print(\"Claim:\\n\\t\" + c)\n",
    "print('\\n\\t* '.join([\"Pro children:\"] + kialo.pros[c]))\n",
    "print('\\n\\t* '.join([\"Con children:\"] + kialo.cons[c]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some limitations of BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we see that \"animal population\" gives quite different results from \"animal populations\". Why is that and how would you fix it?\n",
    "\n",
    "Also, both queries seem to retrieve some claims that are talking about human populations, not animal populations. Why is that and how would you fix it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.closest_claims(\"animal population\", n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kialo.closest_claims(\"Hi Akiko. Are you vegan?\", n=3, kind='has_cons')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A retrieval bot (Akiko)\n",
    "\n",
    "The starter code defines a simple argubot named Akiko (defined in `argubots.py`) that doesn't use an LLM at all. It simply finds a Kialo claim that is similar to what the human just said, and responds with one of the Kialo counterarguments to that claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Watch Akiko argue with Darius. You will first see log messages that show claims that Akiko retrieved, as well as the LLM calls that Darius made. Then you will see the dialog between Akiko and Darius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_cm import LoggingContext\n",
    "from simulate import simulated_dialogue\n",
    "\n",
    "# Have Akiko talk to Darius and spy on the back-end messages to/from the LLM server.\n",
    "with LoggingContext(\"agents\", \"INFO\"):\n",
    "    akiko_darius = simulated_dialogue(argubots.akiko, agents.darius, 6)\n",
    "\n",
    "rich.print(akiko_darius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now talk to Akiko yourself. (Remember that Akiko only knows about subjects that it read about in the `data` directory. If you want to talk about something else, you can add more conversations from [kialo.com]; see the `LICENSE` file.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_cm import LoggingContext\n",
    "with LoggingContext(\"agents\", \"INFO\"):   # temporarily increase logging level\n",
    "    argubots.akiko.converse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "❓❓❓ <b>Task 3: Evaluate Akiko<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you did for Task 2, use the model based evaluation strategy shown in `demo.py`.\n",
    "\n",
    "Then compare Akiko's results to those you got for Airhead and Alice. Who does best? What are the differences in the subscores and comments? Does it matter which character you're evaluating on -- maybe the different characters expoes the bots' various strenghts and weaknesses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the code for your evaluation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-augmented generation (Aragorn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The real weakness of Akiko:\n",
    "\n",
    "* They can only make statements that are already in Kialo.\n",
    "* They don't respond to the user's actual statement, but to a single retrieved Kialo claim that may not accurately reflect the user's position (it just overlaps in words).\n",
    "\n",
    "But we also have access to an LLM, which is able to generate new, contextually appropriate text (as Alice does).\n",
    "\n",
    "In this section, you will create an argubot named Aragorn, who is basically the love child of Akiko and Alice, combining the high-quality specific content of Kialo with the broad competence of an LLM.\n",
    "\n",
    "The RAG in aRAGorn's name stands for **retrieval-augmented generation**. Aragorn is an agent that will take 3 steps to compute its `Agent.response()`:\n",
    "\n",
    "1. **Query formation step:** Ask the LLM what claim should be responded to. For example, consider the following dialogue:\n",
    "\n",
    "    >  ... Aragorn: Fortunately, the vaccine was developed in record time. Human: Sounds fishy.\n",
    "\n",
    "    \"Sounds fishy\" is exactly the kind of statement that Akiko had trouble using as a Kialo query. But Aragorn shows the whole dialogue to the LLM, and asks the LLM what the human's last turn was really saying or implying, in that context. The LLM answers with a much longer statement:\n",
    "\n",
    "    >  Human [paraphrased]: A vaccine that was developed very quickly cannot be trusted. If its developers are claiming that it is safe and effective, I question their motives.\n",
    "\n",
    "    This paraphrase makes an explicit claim and can be better understood without the context. It also contains many more word types, which makes it more likely that BM25 will be able to find a Kialo claim with a nontrivial number of those types.\n",
    "\n",
    "2. **Retrieval step:** Look up claims in Kialo that are similar to the explicit claim. Create a short \"document\" that describes some of those claims and their neighbors on Kialo.\n",
    "\n",
    "3. **Retrieval-augmented generation:** Prompt the LLM to generate the response (like any `LLMAgent`). But include the new \"document\" somewhere in the LLM prompt, in a way that it influences the response.\n",
    "\n",
    "    Thus, the LLM can respond in a way that is appropriate to the dialogue but also draws on the curated information that was retrieved in Kialo. After all, it is a Transformer and can attend to both!\n",
    "\n",
    "Here's an example of the kind of document you might create at the retrieval step, though it may be possible to do better than this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refers to global `kialo` as defined above\n",
    "def kialo_responses(s: str) -> str:\n",
    "    c = kialo.closest_claims(s, kind='has_cons')[0]\n",
    "    result = f'One possibly related claim from the Kialo debate website:\\n\\t\"{c}\"'\n",
    "    if kialo.pros[c]:\n",
    "        result += '\\n' + '\\n\\t* '.join([\"Some arguments from other Kialo users in favor of that claim:\"] + kialo.pros[c])\n",
    "    if kialo.cons[c]:\n",
    "        result += '\\n' + '\\n\\t* '.join([\"Some arguments from other Kialo users against that claim:\"] + kialo.cons[c])\n",
    "    return result\n",
    "        \n",
    "print(kialo_responses(\"Animal flesh is yucky to think about, yet delicious.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "❓❓❓ <b>Task 4: Create Aragorn, an argubot using retrieval-augmented generation<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement Aragorn in `argubots.py` (at the very bottom) as an instance called `aragorn` of a new class `RAGAgent` that is a subclass of `Agent` or `LLMAgent`.\n",
    "\n",
    "Once implemented, you should be able to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_cm import LoggingContext\n",
    "from simulate import simulated_dialogue\n",
    "\n",
    "# Have Aragorn talk to Darius and spy on the back-end messages to/from the LLM server.\n",
    "with LoggingContext(\"agents\", \"INFO\"):\n",
    "    aragorn_darius = simulated_dialogue(argubots.aragorn, agents.darius, 6)\n",
    "\n",
    "rich.print(aragorn_darius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "❓❓❓ <b>Task 5: Evaluate Aragorn<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you did for Task 2, use the model based evaluation strategy shown in `demo.py`.\n",
    "\n",
    "Then compare Aragorn's results to those you got for Airhead and Alice. Who does best? What are the differences in the subscores and comments? Does it matter which character you're evaluating on -- maybe the different characters expoes the bots' various strenghts and weaknesses?\n",
    "\n",
    "Try to figure out how to improve Aragorn's score. Can you beat Alice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the code for your evaluation here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Victor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add another LLM-based argubot to `argubots.py`. Call it ***Victor***. Try to make it get the best score, according to `evaluate.eval_on_characters`.\n",
    "\n",
    "You may want to use Aragorn or Alice as your starting point. Then see if you can find tricks that will get a more awesome score for Victor. How you choose to do that is up to you, but some ideas are suggested below.\n",
    "\n",
    "(Reminder: *Don't change evaluation.* Just build a better argubot.)\n",
    "\n",
    "In your report, you should describe what you did and discuss what you found. If the idea was interesting and you implemented it correctly and well, it's okay if it turns out not to help the evaluation score. Many good ideas don't work. That's why you need to keep finding and trying new good ideas. (Sometimes an idea does help, but in a way that is not picked up by the scoring metric. If that is the case, make sure to discuss this in detail in your report.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "❓❓❓ <b>Task 6: Create Victor<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should implement Victor in `argubots.py` (at the very bottom).\n",
    "\n",
    "Once implemented, you should be able to run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging_cm import LoggingContext\n",
    "from simulate import simulated_dialogue\n",
    "\n",
    "# Have Victor talk to Darius and spy on the back-end messages to/from the LLM server.\n",
    "with LoggingContext(\"agents\", \"INFO\"):\n",
    "    victor_darius = simulated_dialogue(argubots.victor, agents.darius, 6)\n",
    "\n",
    "rich.print(victor_darius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "❓❓❓ <b>Task 7: Evaluate Victor<b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you did for Tasks 2 and 4, use the model based evaluation strategy shown in `demo.py`.\n",
    "\n",
    "Compare Victor's evaluation results to those for the other argubots. And as for Aragorn, dig into the results a bit. Look at the subscores and evaluation comments. Read the simulated dialogs and see whether you can identify patterns of what Victor does well and where they struggle. Did the ideas you had for how to make Victor awesome have the effect you thought they would?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the code for your evaluation here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
